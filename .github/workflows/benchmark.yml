name: Benchmark
on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark'
        default: 'qwen2.5-coder:7b'
        required: true
  push:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
      - '.github/scripts/update_benchmark_docs.sh'
      - '**.rs'
      - Cargo.lock
      - Cargo.toml
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
      - '.github/scripts/update_benchmark_docs.sh'
      - '**.rs'
      - Cargo.lock
      - Cargo.toml

permissions:
  contents: write
  id-token: write
  actions: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Benchmark Tests
    runs-on: [self-hosted, gpu]
    timeout-minutes: 20

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Set model from inputs or default
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "MODEL=${{ github.event.inputs.model }}" >> $GITHUB_ENV
          else
            echo "MODEL=qwen2.5-coder:7b" >> $GITHUB_ENV
          fi

      - name: Check Ollama server
        run: |
          # Check if Ollama server is running and accessible
          echo "Checking connection to Ollama server..."
          if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "Ollama server is accessible"
          else
            echo "Error: Cannot connect to Ollama server at http://localhost:11434"
            exit 1
          fi

          # Verify the model is available
          echo "Verifying model ${{ env.MODEL }} is available..."
          if curl -s http://localhost:11434/api/tags | grep -q "${{ env.MODEL }}"; then
            echo "Model ${{ env.MODEL }} is available"
          else
            echo "Warning: Model ${{ env.MODEL }} may not be available. Continuing anyway as it might be known by a different name."
          fi

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-benchmark-${{ hashFiles('**/Cargo.lock') }}

      - name: Build only what's needed
        run: |
          export RUSTFLAGS="-C codegen-units=16 -C opt-level=1"
          cargo build --release --bin oli-server --features "benchmark"

      - name: Setup test environment
        run: |
          # Create results directory
          mkdir -p benchmark_results/tool_tests

          # Create config for Ollama
          mkdir -p ~/.config/oli
          cat > ~/.config/oli/config.json << EOF
          {
            "default_provider": "ollama",
            "default_model": "${{ env.MODEL }}"
          }
          EOF

          # Set dummy API keys
          echo "ANTHROPIC_API_KEY=dummy-key" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=dummy-key" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=dummy-key" >> $GITHUB_ENV

      - name: Run minimal benchmark test
        id: tool_benchmark
        timeout-minutes: 20
        run: |
          # Set environment variables
          export OLLAMA_API_BASE="http://localhost:11434"
          export DEFAULT_PROVIDER="ollama"
          export DEFAULT_MODEL="${{ env.MODEL }}"
          export OLLAMA_SYSTEM_CONTEXT_LENGTH="4096"
          export OLI_TEST_TIMEOUT="90"
          export OLI_BENCHMARK_SUBSET="true"
          export RUST_LOG="info"

          # Special handling for Qwen models
          if [[ "${{ env.MODEL }}" == *"qwen"* ]]; then
            export OLLAMA_FUNCTION_CALLING_FORMAT="verbose"
          fi

          # Run all benchmark tests in agent::test_tools
          echo "Running all benchmark tests..."
          START=$(date +%s%3N)
          set +e  # Don't exit on error

          # First get the list of all benchmark tests
          BENCH_TESTS=$(grep -A 1 "cfg_attr.*feature.*benchmark.*ignore" $(find ./tests -type f -name "*.rs") 2>/dev/null |
                        grep -o "async fn test[a-zA-Z0-9_]*" |
                        grep "_with_llm" |
                        sed 's/async fn //g')

          # Run tests one by one to get accurate timing for each
          RESULT=""
          TEST_RESULT=0
          for TEST in $BENCH_TESTS; do
            echo "Running benchmark test: $TEST"
            TEST_START=$(date +%s%3N)
            # Run just this one test
            TEST_OUTPUT=$(cargo test --release --features benchmark --test mod -- \
              agent::test_tools::$TEST \
              --exact --test-threads=1 -- --nocapture 2>&1)
            TEST_EXIT=$?
            TEST_END=$(date +%s%3N)
            TEST_DURATION=$((TEST_END - TEST_START))
            # If this test failed, mark the overall result as failed
            if [ $TEST_EXIT -ne 0 ]; then
              TEST_RESULT=1
            fi
            # Add test output to overall result with computed duration
            RESULT+="$TEST_OUTPUT\n"
            RESULT+="Individual test time for $TEST: ${TEST_DURATION}ms ($(echo "scale=2; $TEST_DURATION/1000" | bc)s)\n"
          done

          TEST_EXIT_CODE=$TEST_RESULT
          END=$(date +%s%3N)
          TIME=$((END - START))
          set -e  # Return to exit on error

          # Extract test details
          # Count total benchmark tests and successful tests (based on tests that were actually run)
          TOTAL_TESTS=$(echo "$BENCH_TESTS" | wc -w | tr -d ' ' || echo "0")
          # Count successful tests by examining the result for each test
          SUCCESS_COUNT=0
          for TEST in $BENCH_TESTS; do
            if echo "$RESULT" | grep -q "$TEST.*ok"; then
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            fi
          done

          # Get overall time for all tests combined
          TEST_TIME=$(echo "scale=2; $TIME/1000" | bc)

          # Save results as JSON
          cat > benchmark_results/tool_tests/tools_benchmark_results.json << EOF
          {
            "raw_output": $(echo "$RESULT" | jq -Rs .),
            "test_details": {
              "total_tests": $TOTAL_TESTS,
              "successful_tests": $SUCCESS_COUNT,
              "test_time_seconds": "$TEST_TIME",
              "capabilities": $(
                # Get all benchmark test function names from test pattern (only _with_llm tests)
                ALL_TESTS=$(grep -A 1 "cfg_attr.*feature.*benchmark.*ignore" $(find ./tests -type f -name "*.rs") 2>/dev/null |
                           grep -o "async fn test[a-zA-Z0-9_]*" |
                           grep "_with_llm" |
                           sed 's/async fn //g');
                # Start JSON object
                echo "{";
                # Process each test
                FIRST=true;
                for TEST in $ALL_TESTS; do
                  KEY=$(echo "$TEST" | sed 's/test_//g');
                  # Extract individual test time from our custom output
                  TIME_INFO=$(echo "$RESULT" | grep "Individual test time for $TEST" | grep -o "[0-9]\+ms ([0-9.]\+s)" | head -1 || echo "");
                  # If no time found, try traditional pattern
                  if [ -z "$TIME_INFO" ]; then
                    TIME_PATTERN="$TEST[^(]*(\([0-9.]+s\))";
                    TIME_INFO=$(echo "$RESULT" | grep -o "$TIME_PATTERN" | grep -o "([0-9.]\+s)" || echo "");
                  fi
                  # If still no time found, try another alternate pattern
                  if [ -z "$TIME_INFO" ]; then
                    TIME_INFO=$(echo "$RESULT" | grep "$TEST" | grep -o "finished in [0-9.]\+s" | head -1 | sed 's/finished in //g' || echo "");
                  fi
                  # Check if test passed
                  PASSED=$(if echo "$RESULT" | grep -q "$TEST.*ok"; then echo "true"; else echo "false"; fi);
                  if [ "$FIRST" = true ]; then FIRST=false; else echo ","; fi;
                  echo "\"$KEY\": {\"passed\": $PASSED, \"time\": \"$TIME_INFO\"}";
                done;
                # End JSON object
                echo "}";
                )
            }
          }
          EOF

          # Create summary files
          TEST_STATUS="failed"
          if [ $TEST_EXIT_CODE -eq 0 ]; then
            TEST_STATUS="completed"
          fi

          cat > benchmark_results/tool_tests/summary.json << EOF
          {
            "metadata": {
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "model": "${{ env.MODEL }}",
              "test_type": "tools_benchmark"
            },
            "metrics": {
              "execution_time_ms": $TIME,
              "exit_code": $TEST_EXIT_CODE,
              "success_rate": $(awk "BEGIN {if ($TOTAL_TESTS == 0) print 0; else print ($SUCCESS_COUNT/$TOTAL_TESTS)}")
            },
            "result": "$TEST_STATUS"
          }
          EOF

          cat > benchmark_results/summary.json << EOF
          {
            "tool_benchmark_ms": $TIME,
            "model": "${{ env.MODEL }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "status": "$TEST_STATUS",
            "exit_code": $TEST_EXIT_CODE,
            "test_summary": {
              "total": $TOTAL_TESTS,
              "passed": $SUCCESS_COUNT,
              "success_rate": $(awk "BEGIN {if ($TOTAL_TESTS == 0) print 0; else print ($SUCCESS_COUNT/$TOTAL_TESTS)}")
            }
          }
          EOF

          # Output summary
          echo "=== Benchmark Results ==="
          echo "Model: ${{ env.MODEL }}"
          echo "Duration: ${TIME}ms"
          echo "Status: $TEST_STATUS (exit code: $TEST_EXIT_CODE)"
          echo "Tests: $SUCCESS_COUNT/$TOTAL_TESTS passed"

          # Show which tool tests passed or failed - dynamically generated from available tests
          echo "Tool Tests:"
          # Get all benchmark test names
          ALL_TESTS=$(grep -A 1 "cfg_attr.*feature.*benchmark.*ignore" $(find ./tests -type f -name "*.rs") 2>/dev/null | grep -o "async fn test[a-zA-Z0-9_]*" | sed 's/async fn //g')

          # Format each test result with a checkmark or X
          for TEST in $ALL_TESTS; do
            # Create a user-friendly name (removes test_ prefix and converts _ to space)
            TEST_NAME=$(echo "$TEST" | sed 's/test_//g' | sed 's/_/ /g' | awk '{for(i=1;i<=NF;i++){ $i=toupper(substr($i,1,1)) substr($i,2) }}1')

            # Check if the test passed
            if echo "$RESULT" | grep -q "$TEST.*ok"; then
              echo "✓ $TEST_NAME Tool"
            else
              echo "✗ $TEST_NAME Tool"
            fi
          done

          # Store for other steps
          echo "tool_benchmark_time=${TIME}" >> $GITHUB_OUTPUT
          echo "tool_benchmark_status=${TEST_STATUS}" >> $GITHUB_OUTPUT

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark_results/

  update-docs:
    name: Update Benchmark Documentation
    needs: benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Download benchmark results
        uses: actions/download-artifact@v5
        with:
          name: benchmark-results
          path: benchmark_results/

      - name: Generate updated benchmark docs
        run: |
          chmod +x .github/scripts/update_benchmark_docs.sh
          .github/scripts/update_benchmark_docs.sh

      - name: Check for changes
        id: check_changes
        run: |
          if ! git diff --quiet docs/src/benchmark.md; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Create Pull Request
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.BENCHMARK_PAT }}
          commit-message: "Update benchmark results"
          title: "Update benchmark results from latest run"
          body: |
            This PR updates the benchmark documentation with the latest results.

            - Generated automatically from benchmark run
            - Updated docs/src/benchmark.md with latest metrics
          branch: update-benchmark-docs
          base: main
          delete-branch: true

      - name: Cleanup
        if: always()
        run: echo "Documentation update complete"
